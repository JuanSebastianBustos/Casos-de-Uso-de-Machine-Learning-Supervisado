{% extends "base.html" %}

{% block title %}Conceptos B√°sicos - Regresi√≥n Lineal{% endblock %}

{% block content %}
<section class="rl-conceptos">
    <div class="container">
        <h1>Conceptos B√°sicos de Regresi√≥n Lineal</h1>
        
        <div class="concepto-section">
            <h2>Definici√≥n</h2>
            <p>La regresi√≥n lineal es uno de los m√©todos estad√≠sticos m√°s utilizados en ciencias sociales, econom√≠a, ingenier√≠a y ciencias naturales para analizar la relaci√≥n entre una variable dependiente (tambi√©n llamada variable de respuesta o explicada) y una o varias variables independientes (tambi√©n denominadas predictoras o regresoras).</p>
            <p>Su prop√≥sito principal es explicar y predecir el comportamiento de la variable dependiente a partir de las independientes. Este an√°lisis permite identificar tendencias, estimar par√°metros desconocidos y evaluar la influencia de diferentes factores sobre una variable de inter√©s.</p>
            <p>En su forma m√°s simple, la regresi√≥n lineal simple modela la relaci√≥n entre dos variables mediante una l√≠nea recta, con la ecuaci√≥n:</p>
            <p class="formula">Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX + Œµ</p>
            <p>Donde:</p>
            <ul>
                <li>Y es la variable dependiente</li>
                <li>X es la variable independiente</li>
                <li>Œ≤‚ÇÄ es el intercepto</li>
                <li>Œ≤‚ÇÅ es la pendiente</li>
                <li>Œµ es el t√©rmino de error</li>
            </ul>
            <p>En contextos m√°s complejos se utiliza la regresi√≥n lineal m√∫ltiple, donde se incluyen varias variables independientes:</p>
             <p class="formula">Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX‚ÇÅ +Œ≤‚ÇÇX‚ÇÇ+...+Œ≤‚ÇôX‚Çô+ Œµ</p>
            <p>Esto permite estudiar simult√°neamente el efecto de m√∫ltiples factores sobre la variable dependiente.</p>
        </div>

        <div class="concepto-section">
            <h2>Supuestos habituales</h2>
            <p>Para que la regresi√≥n lineal sea estad√≠sticamente v√°lida y produzca estimaciones confiables e interpretaciones correctas, es necesario que se cumplan ciertos supuestos te√≥ricos. Estos supuestos son la base del m√©todo de M√≠nimos Cuadrados Ordinarios (MCO), que se emplea para estimar los coeficientes ùõΩ.</p>
            <ul>
                <li><strong>Linealidad:</strong> </li>
                <ul>
                   <li>El modelo asume que la relaci√≥n entre la variable dependiente y cada una de las independientes es lineal.</li> 
                   <li>Si la relaci√≥n es curva, exponencial o polin√≥mica, el modelo lineal no ser√° el m√°s adecuado.</li>
                   <li>Una violaci√≥n de este supuesto puede llevar a predicciones sesgadas.</li>
                   <li>Para verificarlo, se suelen usar:Gr√°ficos de dispersi√≥n entre Y y cada X,Gr√°ficos de residuos contra valores ajustados,M√©todos de ajuste de polinomios o transformaciones (logaritmos, ra√≠ces, etc.)</li>
                </u>
                <li><strong>Independencia:</strong> </li>
                    <ul>
                    <li>Los errores (Œµ) deben ser independientes entre s√≠.</li>
                    <li>Esto significa que el error de una observaci√≥n no debe proporcionar informaci√≥n sobre el error de otra.</li>
                    <li>La violaci√≥n m√°s com√∫n ocurre en datos temporales o espaciales, donde puede existir autocorrelaci√≥n.</li>
                    <li>Ejemplo: en un an√°lisis de ventas mensuales, el error de enero puede estar relacionado con el de febrero.</li>
                    <li>La prueba m√°s usada para evaluar este supuesto es el estad√≠stico de Durbin-Watson.</li>
                    <li>Si no se cumple, se pueden emplear modelos para series temporales (como ARIMA) o ajustar errores est√°ndar robustos.</li>
                    </u>
                    
                <li><strong>Homocedasticidad:</strong> </li>
                        <u>
                            <li>Se espera que los errores tengan varianza constante a lo largo de los valores de X.</li>
                            <li>En otras palabras, el grado de dispersi√≥n de los residuos debe ser similar en todo el rango de la variable independiente</li>
                            <li>Cuando este supuesto se viola y la varianza de los errores cambia seg√∫n el valor de X, se presenta heterocedasticidad</li>
                            <li>Consecuencias de la heterocedasticidad:Los estimadores siguen siendo insesgados, pero dejan de ser eficientes,Los intervalos de confianza y pruebas de hip√≥tesis pueden ser incorrectos. </li>
                            <li>Se puede detectar mediante:Gr√°ficos de residuos vs. valores ajustados,Pruebas estad√≠sticas como Breusch-Pagan o White.</li>
                            <li>En caso de heterocedasticidad, se pueden aplicar transformaciones (logaritmo, ra√≠z cuadrada) o usar m√©todos robustos (MCO robustos, GLS).</li>
                        </u>
                <li><strong>Normalidad:</strong> </li>
                        <u>
                            <li>Se asume que los errores siguen una distribuci√≥n normal con media cero.</li>
                            <li>Este supuesto es clave cuando se desea realizar inferencia estad√≠stica, es decir:Construcci√≥n de intervalos de confianza,Realizaci√≥n de pruebas de hip√≥tesis sobre los coeficientes.</li>
                            <li>La normalidad no es tan crucial para la estimaci√≥n de los coeficientes, pero s√≠ para la validez de las pruebas estad√≠sticas.</li>
                            <li>Puede verificarse con: Histogramas de residuos,graficos Q-Q(Quamtile-Quantile) y Pruebas como Shapiro-Wilk, Kolmogorov-Smirnov o Jarque-Bera.</li>
                            <li>Si no se cumple, se pueden usar m√©todos no param√©tricos o transformaciones de variables.</li>
                        </u>
                <li><strong>No multicolinealidad:</strong> En regresi√≥n m√∫ltiple, las variables independientes no est√°n altamente correlacionadas entre s√≠.</li>
            </ul>
        </div>

        <div class="references">
            <h2>Referencias APA</h2>
            <p>James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Springer.</p>
            <p>Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.</p>
            <p>Kutner, M. H., Nachtsheim, C. J., Neter, J., & Li, W. (2005). Applied Linear Statistical Models. McGraw-Hill Irwin.</p>
        </div>
    </div>
</section>
{% endblock %}
